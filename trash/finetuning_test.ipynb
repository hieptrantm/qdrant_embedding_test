{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d20b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.37.0\n",
      "  Downloading transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
      "Collecting tqdm==4.66.1\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting datasets==2.14.6\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers==4.37.0) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.37.0)\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers==4.37.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers==4.37.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers==4.37.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers==4.37.0) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers==4.37.0) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.0)\n",
      "  Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.37.0)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm==4.66.1) (0.4.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (16.1.0)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.6)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (2.2.2)\n",
      "Collecting xxhash (from datasets==2.14.6)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets==2.14.6)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from datasets==2.14.6) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.14.6) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers==4.37.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers==4.37.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers==4.37.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers==4.37.0) (2024.8.30)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.14.6)\n",
      "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->datasets==2.14.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->datasets==2.14.6) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->datasets==2.14.6) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hiep.tranvan\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.16.0)\n",
      "Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
      "   ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 5.5/8.4 MB 33.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.4/8.4 MB 28.9 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -------------------------------------- - 2.1/2.2 MB 59.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, tqdm, safetensors, fsspec, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "Successfully installed datasets-2.14.6 dill-0.3.7 fsspec-2023.10.0 huggingface-hub-0.33.0 multiprocess-0.70.15 safetensors-0.5.3 tokenizers-0.15.2 tqdm-4.66.1 transformers-4.37.0 xxhash-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2023.10.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.37.0 tqdm==4.66.1 datasets==2.14.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49615a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    XLMRobertaForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9021ae8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Liệu các mô hình ngôn ngữ lớn có thể đạt đến mức độ hiểu biết và sáng tạo tương đương con người không, và những rủi ro đạo đức nào cần được xem xét?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Mạng nơ-ron tích chập (Convolutional Neural Networks - CNN) hoạt động như thế nào trong việc xử lý ảnh, và những cải tiến mới nhất trong kiến trúc CNN là gì?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Thuyết tương đối rộng của Einstein tiên đoán sự tồn tại của lỗ đen, vậy bằng chứng thực nghiệm nào đã xác nhận điều này và những hệ quả vật lý nào phát sinh từ đó?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Liệu các mô hình Deep Learning có thể giải quyết được bài toán P=NP không?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Những đột phá nào trong vật lý học có khả năng thay đổi cách chúng ta hiểu về vũ trụ trong 5 năm tới?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Làm thế nào để giảm thiểu các rủi ro đạo đức liên quan đến việc phát triển và triển khai các hệ thống Trí tuệ nhân tạo?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Ứng dụng của mạng nơ-ron tích chập (Convolutional Neural Networks) trong việc xử lý ảnh y tế là gì?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Lý thuyết tương đối rộng của Einstein dự đoán điều gì về sự tồn tại của lỗ đen và sóng hấp dẫn?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Những thách thức đạo đức nào đặt ra khi phát triển các hệ thống trí tuệ nhân tạo tự hành?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Làm thế nào các mô hình Deep Learning có thể được sử dụng để cải thiện chẩn đoán y tế?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Lý thuyết tương đối rộng của Einstein đã thay đổi hiểu biết của chúng ta về vũ trụ như thế nào?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': \"Các phương pháp nào đang được sử dụng để giải quyết vấn đề 'hộp đen' trong Deep Learning và làm cho các quyết định của mô hình dễ hiểu hơn?\",\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Ảnh hưởng của hiệu ứng lượng tử lên các hệ thống vĩ mô là gì và chúng có thể được khai thác như thế nào?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Các ứng dụng tiềm năng của trí tuệ nhân tạo trong việc phát triển năng lượng tái tạo là gì?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Sự khác biệt chính giữa học tăng cường và học có giám sát trong Deep Learning là gì?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Các hạt cơ bản nào tạo nên vật chất và lực tương tác giữa chúng hoạt động như thế nào?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Liệu các mô hình Deep Learning hiện tại có thể đạt đến mức độ hiểu và sáng tạo tương đương con người trong tương lai gần không?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Những thách thức đạo đức nào nảy sinh khi trí tuệ nhân tạo được sử dụng rộng rãi trong việc ra quyết định, đặc biệt là trong các lĩnh vực như tư pháp và y tế?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Làm thế nào các nhà khoa học vật lý có thể chứng minh hoặc bác bỏ sự tồn tại của vật chất tối bằng các thí nghiệm và quan sát hiện tại?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Các mô hình Deep Learning có thể được sử dụng để phát hiện gian lận trong giao dịch tài chính như thế nào?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Nguyên lý bất định Heisenberg ảnh hưởng đến độ chính xác của các phép đo lượng tử như thế nào?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Những thách thức đạo đức nào phát sinh khi sử dụng trí tuệ nhân tạo trong việc ra quyết định pháp luật?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Sự khác biệt chính giữa mạng nơ-ron tích chập (CNN) và mạng nơ-ron hồi quy (RNN) là gì?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Thuyết tương đối rộng của Einstein dự đoán những hiện tượng vật lý nào và chúng đã được chứng minh ra sao?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Làm thế nào AI có thể được sử dụng để cải thiện khả năng tiếp cận giáo dục cho những người khuyết tật?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Làm thế nào các mô hình Deep Learning có thể được sử dụng để phát hiện gian lận trong giao dịch tài chính?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Những tiến bộ gần đây trong vật lý hạt nhân có thể dẫn đến những đột phá nào trong công nghệ năng lượng?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Những thách thức đạo đức nào nảy sinh khi triển khai các hệ thống trí tuệ nhân tạo tự trị trong lĩnh vực chăm sóc sức khỏe?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Làm thế nào để xây dựng một mạng neural deep learning hiệu quả cho bài toán phân loại ảnh?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Liệu có thể tạo ra một AI có khả năng tự nhận thức và cảm xúc tương tự con người không?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Thuyết tương đối rộng của Einstein giải thích sự hình thành và tiến hóa của vũ trụ như thế nào?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Ứng dụng của deep learning trong việc phát hiện gian lận tài chính là gì?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Những thách thức đạo đức nào nảy sinh khi AI ngày càng trở nên mạnh mẽ và tự động hơn?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Lỗ đen ảnh hưởng đến không gian và thời gian xung quanh nó như thế nào theo vật lý học?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Các mô hình Deep Learning hiện tại có những hạn chế nào trong việc xử lý ngôn ngữ tự nhiên?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Liệu có thể tạo ra một thuật toán AI có khả năng tự nhận thức và tư duy sáng tạo như con người không?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Lý thuyết tương đối của Einstein đã thay đổi hiểu biết của chúng ta về vũ trụ như thế nào?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Liệu các mạng nơ-ron sâu có thể tự học hỏi và sáng tạo ra những tác phẩm nghệ thuật độc đáo hay không?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Những đột phá mới nhất trong lĩnh vực vật lý lượng tử có thể dẫn đến việc phát triển công nghệ du hành thời gian hay không?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Những thách thức đạo đức nào cần được xem xét khi trí tuệ nhân tạo ngày càng trở nên mạnh mẽ và tự chủ hơn?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Liệu các thuật toán Deep Learning có thể giải quyết được vấn đề biến đổi khí hậu hiệu quả hơn các phương pháp truyền thống không?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Lỗ đen có thực sự bẻ cong không gian và thời gian như thuyết tương đối rộng dự đoán không, và bằng chứng thực nghiệm nào chứng minh điều đó?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'},\n",
       " {'content': 'Những rủi ro đạo đức và xã hội nào cần được xem xét khi phát triển các hệ thống Trí tuệ nhân tạo tự hành?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Trí tuệ nhân tạo'},\n",
       " {'content': 'Làm thế nào các mô hình Deep Learning có thể được sử dụng để cải thiện khả năng chẩn đoán bệnh trong y học?',\n",
       "  'folder': 'technology',\n",
       "  'topic': 'Deep Learning'},\n",
       " {'content': 'Thuyết lượng tử có ảnh hưởng như thế nào đến sự hiểu biết của chúng ta về vũ trụ?',\n",
       "  'folder': 'science',\n",
       "  'topic': 'Vật lý học'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data/filter_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe8163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoder\n",
    "label2id = {\n",
    "    (\"technology\", \"Trí tuệ nhân tạo\"): 0,\n",
    "    (\"technology\", \"Deep Learning\"): 1,\n",
    "    (\"science\", \"Vật lý học\"): 2\n",
    "}\n",
    "\n",
    "id2label =  {v: k for k, v in label2id.items()}\n",
    "train_label_ids = [label2id[(d['folder'], d['topic'])] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d6c1858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hiep.tranvan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hiep.tranvan\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# tokenzier\n",
    "#tokenizer roberta using AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "encodings = tokenizer(\n",
    "    [d['content'] for d in data], \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6acd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MyDataset(encodings, train_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "089bd793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hiep.tranvan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hiep.tranvan\\.cache\\huggingface\\hub\\models--FacebookAI--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"FacebookAI/xlm-roberta-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96dc9a59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#training \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m trainer = Trainer(\n\u001b[32m     13\u001b[39m     model=model,\n\u001b[32m     14\u001b[39m     args=training_args,\n\u001b[32m     15\u001b[39m     train_dataset=train_dataset\n\u001b[32m     16\u001b[39m )\n\u001b[32m     17\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:131\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiep.tranvan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1738\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1736\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1738\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[32m   1741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.average_tokens_across_devices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiep.tranvan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2268\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2264\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2265\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2266\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2267\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiep.tranvan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\generic.py:67\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     65\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiep.tranvan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2138\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2139\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2140\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2141\u001b[39m         )\n\u001b[32m   2142\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2143\u001b[39m accelerator_state_kwargs = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "#training \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=1,\n",
    "    save_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dự đoán thử\n",
    "test_text = \"Học sâu là gì?\"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "outputs = model(**inputs)\n",
    "pred_label_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "pred_folder, pred_topic = id2label[pred_label_id]\n",
    "print(f\"Folder: {pred_folder}, Topic: {pred_topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e62b567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filepath = 'trash/query.json'\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb44070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'liệu ai có thể thay thế vai trò của giáo viên trong giáo dục'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['query']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
